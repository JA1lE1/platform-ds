# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

# JupyterHub docker-compose configuration file
version: '3.1'

services:
  hub-db:
    image: postgres:9.5
    container_name: jupyterhub-db
    restart: always
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      PGDATA: ${DB_VOLUME_CONTAINER}
    env_file:
      - jupyterhub/secrets/postgres.env
    volumes:
      - "db:${DB_VOLUME_CONTAINER}"

  hub:
    depends_on:
      - hub-db
    build:
      context: ./jupyterhub
      dockerfile: Dockerfile
      args:
        JUPYTERHUB_VERSION: ${JUPYTERHUB_VERSION}
        http_proxy:
        https_proxy:
        ftp_proxy:
    restart: always
    image: jupyterhub
    container_name: jupyterhub
    volumes:
      # Bind Docker socket on the host so we can connect to the daemon from
      # within the container
      - "/var/run/docker.sock:/var/run/docker.sock:rw"
      - "/home/public/nbviewer:/public"
      # Bind Docker volume on host for JupyterHub database and cookie secrets
      - "data:${DATA_VOLUME_CONTAINER}"
      - "/etc/shadow:/etc/shadow"
      - "/etc/passwd:/etc/passwd"
      - "/etc/pam.d:/etc/pam.d"
    ports:
      - "443:443"
      - "7070:7070"
    links:
      - hub-db
    environment:
      HOST:
      # All containers will join this network
      DOCKER_NETWORK_NAME: ${DOCKER_NETWORK_NAME}
      # JupyterHub will spawn this Notebook image for users
      DOCKER_NOTEBOOK_IMAGE: ${LOCAL_NOTEBOOK_IMAGE}
      # Notebook directory inside user image
      DOCKER_NOTEBOOK_DIR: ${DOCKER_NOTEBOOK_DIR}
      # Using this run command (optional)
      DOCKER_SPAWN_CMD: ${DOCKER_SPAWN_CMD}
      ## needed for nvidia-docker version 2
      NVIDIA_VISIBLE_DEVICES:     all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Postgres db info
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST: hub-db
    env_file:
      - jupyterhub/secrets/postgres.env
      - jupyterhub/secrets/oauth.env
    command: >
      jupyterhub -f /srv/jupyterhub/jupyterhub_config.py

  spark-master:
    build:
      context: ./spark-master
      dockerfile: Dockerfile
      args:
        http_proxy:
        https_proxy:
        ftp_proxy:
    image: spark-master:2.4.3
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8585:8080"
      - "7077:7077"
    volumes:
      - /mnt/spark-apps:/opt/spark-apps
      - /mnt/spark-data:/opt/spark-data
            
  spark-worker-1:
    build:
      context: ./spark-worker
      dockerfile: Dockerfile
      args:
        http_proxy:
        https_proxy:
        ftp_proxy:
    image: spark-worker:2.4.3
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
       - /mnt/spark-apps:/opt/spark-apps
       - /mnt/spark-data:/opt/spark-data

  spark-worker-2:
    depends_on:
      - spark-worker-1
    image: spark-worker:2.4.3
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    volumes:
       - /mnt/spark-apps:/opt/spark-apps
       - /mnt/spark-data:/opt/spark-data

  spark-worker-3:
    depends_on:
      - spark-worker-1 
    image: spark-worker:2.4.3
    container_name: spark-worker-3
    hostname: spark-worker-3
    depends_on:
      - spark-master
    ports:
      - "8083:8081"
    volumes:
       - /mnt/spark-apps:/opt/spark-apps
       - /mnt/spark-data:/opt/spark-data
  
  namenode:
    image: platform-ds/hadoop-namenode:2.0.0-hadoop3.1.1-java8
    container_name: namenode
    build:
      context: ./namenode
      dockerfile: Dockerfile
      args:
        http_proxy:
        https_proxy:
        ftp_proxy:
    volumes:
      - "namenode:/hadoop/dfs/name"
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 9870:9870
      - 50070:50070

  datanode:
    image: platform-ds/hadoop-datanode:2.0.0-hadoop3.1.1-java8
    build:
      context: ./datanode
      dockerfile: Dockerfile
      args:
        http_proxy:
        https_proxy:
        ftp_proxy:
    container_name: datanode
    depends_on:
      - namenode
    volumes:
      - "datanode:/hadoop/dfs/data"
    env_file:
      - ./hadoop.env
    ports:
      - 9864:9864
      - 50075:50075
    
volumes:
  datanode:
    external:
      name: datanode
  namenode:
    external:
      name: namenode
  data:
    external:
      name: "${DATA_VOLUME_HOST}"
  db:
    external:
      name: "${DB_VOLUME_HOST}"

networks:
  default:
    external:
      name: ${DOCKER_NETWORK_NAME}
